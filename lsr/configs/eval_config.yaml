defaults:
  - _self_ 
  - model: splade
  - dataset@eval_dataset: toy_triplets
  - loss: triplet_margin
  - wandb: defaults

query_max_length: 250
doc_max_length: 512
tokenizer:
  _target_: lsr.tokenizer.HFTokenizer
  tokenizer_name: distilbert-base-uncased

data_collator:
  _target_: lsr.datasets.data_collator.DataCollator
  tokenizer: ${tokenizer}
  q_max_length: ${query_max_length}
  d_max_length: ${doc_max_length}

training_arguments:
  _target_: transformers.TrainingArguments
  overwrite_output_dir: True 
  remove_unused_columns: False 
  do_train: True
  evaluation_strategy: 'no'
  log_level: info 
  logging_steps: 500
  per_device_train_batch_size: 128
  max_steps: 300000
  save_total_limit: 2 
  # num_train_epochs: 30
  save_strategy: "steps" 
  save_steps: 20000
  warmup_steps: 6000
  fp16: True
  report_to: wandb 
  dataloader_num_workers: 16
  dataloader_drop_last: False
  ignore_data_skip: False
  ddp_find_unused_parameters: False
  seed: 42

trainer: 
  _target_: lsr.trainer.HFTrainer
  model: ${model}
  args: ${training_arguments}
  data_collator: ${data_collator}
  train_dataset: ${train_dataset}
  loss: ${loss}
  
hydra:
  job:
    chdir: False
resume_from_checkpoint: False 